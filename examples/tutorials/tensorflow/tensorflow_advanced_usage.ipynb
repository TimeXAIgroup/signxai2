{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SignXAI with TensorFlow - Advanced Usage\n",
    "\n",
    "This tutorial demonstrates advanced techniques for using SignXAI with TensorFlow models. It builds on the basic tutorial and explores more sophisticated explainability methods and customizations.\n",
    "\n",
    "## Setup Requirements\n",
    "\n",
    "For this TensorFlow tutorial, you'll need to install SignXAI with TensorFlow dependencies:\n",
    "\n",
    "```bash\n",
    "# For conda users\n",
    "conda create -n signxai-tensorflow python=3.8\n",
    "conda activate signxai-tensorflow\n",
    "pip install -r ../../requirements/common.txt\n",
    "pip install -r ../../requirements/tensorflow.txt\n",
    "\n",
    "# Or for pip users\n",
    "python -m venv signxai_tensorflow_env\n",
    "source signxai_tensorflow_env/bin/activate  # On Windows: signxai_tensorflow_env\\Scripts\\activate\n",
    "pip install -r ../../requirements/common.txt\n",
    "pip install -r ../../requirements/tensorflow.txt\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "1. Customizing LRP rules with TensorFlow\n",
    "2. Creating custom explainability methods\n",
    "3. Working with custom model architectures\n",
    "4. Advanced visualization techniques\n",
    "5. Integrating with TensorFlow's GradientTape API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# SignXAI imports - including advanced modules\n",
    "from signxai.tf_signxai.methods import SIGN, GradCAM, GuidedBackprop\n",
    "from signxai.tf_signxai.methods.innvestigate.analyzer import LRPZPlus, LRPEpsilon, LRPAlpha1Beta0\n",
    "from signxai.common.visualization import visualize_attribution\n",
    "from signxai.utils.utils import remove_softmax, load_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Paths and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and model paths\n",
    "_THIS_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "DATA_DIR = os.path.realpath(os.path.join(_THIS_DIR, \"..\", \"..\", \"data\"))\n",
    "VGG16_MODEL_PATH = os.path.join(DATA_DIR, \"models\", \"tensorflow\", \"VGG16\", \"vgg16_tf.h5\")\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, \"images\", \"example.jpg\")\n",
    "\n",
    "# Verify paths\n",
    "assert os.path.exists(VGG16_MODEL_PATH), f\"VGG16 model not found at {VGG16_MODEL_PATH}\"\n",
    "assert os.path.exists(IMAGE_PATH), f\"Image not found at {IMAGE_PATH}\"\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = tf.keras.models.load_model(VGG16_MODEL_PATH)\n",
    "    print(\"Loaded pre-saved VGG16 model\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load saved model: {e}\\nLoading from Keras applications instead\")\n",
    "    model = tf.keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "    \n",
    "# Remove softmax for explanation methods\n",
    "model_no_softmax = remove_softmax(model)\n",
    "\n",
    "# Load and display image\n",
    "original_img, preprocessed_img = load_image(IMAGE_PATH, expand_dims=True)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(original_img)\n",
    "plt.axis('off')\n",
    "plt.title('Sample Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customizing LRP Rules with TensorFlow\n",
    "\n",
    "SignXAI provides extensive customization options for Layer-wise Relevance Propagation (LRP) with TensorFlow models. Let's explore different LRP rules and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prediction for reference\n",
    "predictions = model.predict(preprocessed_img)\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "print(f\"Predicted class index: {predicted_class}\")\n",
    "\n",
    "# Let's import the specialized analyzer creation function\n",
    "from signxai.tf_signxai.methods.innvestigate.analyzer import create_analyzer\n",
    "\n",
    "# Create LRP analyzers with different configurations\n",
    "# 1. LRP-Z\n",
    "lrp_z_analyzer = create_analyzer('lrp.z', model_no_softmax)\n",
    "\n",
    "# 2. LRP-Epsilon with custom epsilon\n",
    "epsilon = 0.01\n",
    "lrp_epsilon_analyzer = create_analyzer('lrp.epsilon', model_no_softmax, epsilon=epsilon)\n",
    "\n",
    "# 3. LRP-Alpha-Beta with fixed alpha=1, beta=0\n",
    "lrp_alpha_beta_analyzer = create_analyzer('lrp.alpha_1_beta_0', model_no_softmax)\n",
    "\n",
    "# 4. LRP with composite rules - different rules for different layers\n",
    "lrp_composite_analyzer = create_analyzer('lrp.sequential_composite_a', model_no_softmax)\n",
    "\n",
    "# 5. LRP with custom input layer rule\n",
    "lrp_custom_input = create_analyzer('lrp.z', model_no_softmax, input_layer_rule='SIGN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get explanations from analyzers\n",
    "def get_lrp_explanation(analyzer, input_img, class_idx=None):\n",
    "    # Default to predicted class if none provided\n",
    "    if class_idx is None:\n",
    "        class_idx = predicted_class\n",
    "        \n",
    "    # Use the analyzer to get explanation\n",
    "    explanation = analyzer.analyze([input_img[0]], neuron_selection=class_idx)\n",
    "    \n",
    "    # Ensure we get the actual explanation (might be in a dict)\n",
    "    if isinstance(explanation, dict):\n",
    "        explanation = explanation[list(explanation.keys())[0]][0]\n",
    "    else:\n",
    "        explanation = explanation[0]\n",
    "    \n",
    "    return explanation\n",
    "\n",
    "# Generate explanations for each LRP variant\n",
    "lrp_explanations = {\n",
    "    \"LRP-Z\": get_lrp_explanation(lrp_z_analyzer, preprocessed_img),\n",
    "    f\"LRP-Epsilon (ε={epsilon})\": get_lrp_explanation(lrp_epsilon_analyzer, preprocessed_img),\n",
    "    \"LRP-Alpha1-Beta0\": get_lrp_explanation(lrp_alpha_beta_analyzer, preprocessed_img),\n",
    "    \"LRP-Composite\": get_lrp_explanation(lrp_composite_analyzer, preprocessed_img),\n",
    "    \"LRP-Z with SIGN input\": get_lrp_explanation(lrp_custom_input, preprocessed_img)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for visualization\n",
    "def preprocess_explanation(explanation):\n",
    "    # Use absolute values for attribution\n",
    "    abs_explanation = np.abs(explanation)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    if abs_explanation.max() > 0:\n",
    "        abs_explanation = abs_explanation / abs_explanation.max()\n",
    "    \n",
    "    return abs_explanation\n",
    "\n",
    "# Visualize all LRP variants\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get numpy array of original image for visualization\n",
    "np_img = np.array(original_img)\n",
    "\n",
    "# Plot the original image in the first position\n",
    "axes[0].imshow(np_img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot all LRP variants in the remaining positions\n",
    "for i, (method_name, explanation) in enumerate(lrp_explanations.items(), 1):\n",
    "    if i < len(axes):\n",
    "        visualize_attribution(np_img, preprocess_explanation(explanation), ax=axes[i])\n",
    "        axes[i].set_title(method_name)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Custom Explainability Methods\n",
    "\n",
    "SignXAI allows you to create your own custom explainability methods or modify existing ones. Let's implement a simple custom method and integrate it with SignXAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a custom method that combines gradient-based methods with an activation threshold\n",
    "class ThresholdedGradient:\n",
    "    def __init__(self, model, threshold=0.2):\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def attribute(self, input_tensor, target_class=None):\n",
    "        # Convert to TensorFlow tensor if necessary\n",
    "        if not isinstance(input_tensor, tf.Tensor):\n",
    "            input_tensor = tf.convert_to_tensor(input_tensor)\n",
    "        \n",
    "        # Make a copy that we can use with gradient tape\n",
    "        input_tensor = tf.identity(input_tensor)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Watch the input tensor\n",
    "            tape.watch(input_tensor)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = self.model(input_tensor)\n",
    "            \n",
    "            # If target class not specified, use the predicted class\n",
    "            if target_class is None:\n",
    "                target_class = tf.argmax(predictions[0])\n",
    "                \n",
    "            # Get the target output\n",
    "            target_output = predictions[:, target_class]\n",
    "            \n",
    "        # Calculate gradients\n",
    "        gradients = tape.gradient(target_output, input_tensor)\n",
    "        \n",
    "        # Convert to numpy for further processing\n",
    "        gradients_np = gradients.numpy()\n",
    "        input_np = input_tensor.numpy()\n",
    "        \n",
    "        # Apply our custom thresholding logic\n",
    "        # Only keep gradients where the absolute value of the input is above threshold\n",
    "        normalized_input = input_np / np.max(np.abs(input_np) + 1e-10)\n",
    "        mask = (np.abs(normalized_input) > self.threshold).astype(float)\n",
    "        \n",
    "        # Apply the mask to the gradients\n",
    "        thresholded_gradients = gradients_np * mask\n",
    "        \n",
    "        return thresholded_gradients\n",
    "\n",
    "# Create our custom explainer\n",
    "custom_explainer = ThresholdedGradient(model_no_softmax, threshold=0.2)\n",
    "\n",
    "# Generate an explanation\n",
    "custom_explanation = custom_explainer.attribute(preprocessed_img, target_class=predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare our custom method with standard gradient and gradient × input\n",
    "# Standard gradient method\n",
    "gradient_explainer = SIGN(model_no_softmax)\n",
    "gradient_explanation = gradient_explainer.attribute(preprocessed_img, target_class=predicted_class).numpy()\n",
    "\n",
    "# Gradient × input method (manually implemented)\n",
    "gradient_x_input = gradient_explanation * preprocessed_img[0].numpy()\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot standard gradient\n",
    "visualize_attribution(np_img, preprocess_explanation(gradient_explanation), ax=axes[0])\n",
    "axes[0].set_title(\"Standard Gradient\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot gradient × input\n",
    "visualize_attribution(np_img, preprocess_explanation(gradient_x_input), ax=axes[1])\n",
    "axes[1].set_title(\"Gradient × Input\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Plot our custom thresholded gradient\n",
    "visualize_attribution(np_img, preprocess_explanation(custom_explanation), ax=axes[2])\n",
    "axes[2].set_title(\"Custom Thresholded Gradient\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Working with Custom Model Architectures\n",
    "\n",
    "SignXAI works with custom TensorFlow model architectures, not just pre-defined ones. Let's create a simple custom model and explain its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple custom CNN model\n",
    "def create_custom_model(input_shape=(224, 224, 3), num_classes=1000):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Feature extraction\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Classification\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Create the custom model\n",
    "custom_model = create_custom_model()\n",
    "custom_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Display the model architecture\n",
    "custom_model.summary()\n",
    "\n",
    "# Remove softmax for explainability\n",
    "custom_model_no_softmax = remove_softmax(custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the custom model\n",
    "# (Since we didn't train it, it'll just make random predictions)\n",
    "custom_prediction = custom_model.predict(preprocessed_img)\n",
    "custom_predicted_class = np.argmax(custom_prediction[0])\n",
    "print(f\"Custom model prediction: Class {custom_predicted_class}\")\n",
    "\n",
    "# Apply different explainability methods to the custom model\n",
    "methods = {\n",
    "    \"Gradient\": SIGN(custom_model_no_softmax),\n",
    "    \"GradCAM\": GradCAM(custom_model, layer_name='conv2d_2'),  # Last conv layer\n",
    "    \"GuidedBackprop\": GuidedBackprop(custom_model_no_softmax),\n",
    "}\n",
    "\n",
    "custom_explanations = {}\n",
    "for name, method in methods.items():\n",
    "    explanation = method.attribute(preprocessed_img, target_class=custom_predicted_class).numpy()\n",
    "    custom_explanations[name] = explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explanations for the custom model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, (name, explanation) in enumerate(custom_explanations.items()):\n",
    "    visualize_attribution(np_img, preprocess_explanation(explanation), ax=axes[i])\n",
    "    axes[i].set_title(f\"Custom Model: {name}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Visualization Techniques\n",
    "\n",
    "SignXAI provides advanced visualization options to help interpret complex models more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a high-quality explanation to visualize\n",
    "lrp_explanation = get_lrp_explanation(lrp_z_analyzer, preprocessed_img)\n",
    "lrp_processed = preprocess_explanation(lrp_explanation)\n",
    "\n",
    "# 1. Visualization with different color maps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Default Red-Blue colormap\n",
    "visualize_attribution(np_img, lrp_processed, ax=axes[0], cmap='RdBu_r')\n",
    "axes[0].set_title(\"Red-Blue Colormap\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Heat colormap\n",
    "visualize_attribution(np_img, lrp_processed, ax=axes[1], cmap='hot')\n",
    "axes[1].set_title(\"Heat Colormap\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Viridis colormap\n",
    "visualize_attribution(np_img, lrp_processed, ax=axes[2], cmap='viridis')\n",
    "axes[2].set_title(\"Viridis Colormap\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Advanced: Creating an occlusion sensitivity map\n",
    "def occlusion_sensitivity(model, image, target_class, patch_size=20, stride=10):\n",
    "    \"\"\"Generate occlusion sensitivity map by sliding a gray patch over the image.\"\"\"\n",
    "    # Get the image dimensions\n",
    "    h, w, c = image.shape\n",
    "    \n",
    "    # Create a baseline prediction\n",
    "    baseline_prediction = model.predict(np.expand_dims(image, axis=0))[0, target_class]\n",
    "    \n",
    "    # Initialize sensitivity map\n",
    "    sensitivity_map = np.zeros((h, w))\n",
    "    \n",
    "    # Create a gray occlusion patch\n",
    "    occlusion_value = 128  # Gray value\n",
    "    \n",
    "    # Loop through the image\n",
    "    for y in range(0, h - patch_size + 1, stride):\n",
    "        for x in range(0, w - patch_size + 1, stride):\n",
    "            # Create a copy of the image\n",
    "            occluded_image = image.copy()\n",
    "            \n",
    "            # Apply the occlusion patch\n",
    "            occluded_image[y:y+patch_size, x:x+patch_size, :] = occlusion_value\n",
    "            \n",
    "            # Get the prediction for the occluded image\n",
    "            occluded_prediction = model.predict(np.expand_dims(occluded_image, axis=0))[0, target_class]\n",
    "            \n",
    "            # Calculate change in prediction\n",
    "            diff = baseline_prediction - occluded_prediction\n",
    "            \n",
    "            # Update the sensitivity map\n",
    "            sensitivity_map[y:y+patch_size, x:x+patch_size] += diff\n",
    "    \n",
    "    # Normalize the sensitivity map\n",
    "    if sensitivity_map.max() != sensitivity_map.min():\n",
    "        sensitivity_map = (sensitivity_map - sensitivity_map.min()) / (sensitivity_map.max() - sensitivity_map.min())\n",
    "    \n",
    "    return sensitivity_map\n",
    "\n",
    "# Generate occlusion sensitivity map (this may take a while)\n",
    "print(\"Generating occlusion sensitivity map (this may take a few minutes)...\")\n",
    "# Using a smaller stride for faster computation in this example\n",
    "occlusion_map = occlusion_sensitivity(model, preprocessed_img[0], predicted_class, \n",
    "                                     patch_size=40, stride=20)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the occlusion sensitivity map\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(np_img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Occlusion sensitivity map\n",
    "axes[1].imshow(occlusion_map, cmap='hot')\n",
    "axes[1].set_title(\"Occlusion Sensitivity Map\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Overlay on original image\n",
    "visualize_attribution(np_img, occlusion_map, ax=axes[2])\n",
    "axes[2].set_title(\"Overlay on Original Image\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integrating with TensorFlow's GradientTape API\n",
    "\n",
    "You can create your own custom explainability methods using TensorFlow's GradientTape for more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement Integrated Gradients manually using GradientTape\n",
    "def integrated_gradients(model, input_image, target_class=None, steps=50, baseline=None):\n",
    "    \"\"\"Implement Integrated Gradients explainability method.\"\"\"\n",
    "    # Convert to TensorFlow tensor if needed\n",
    "    if not isinstance(input_image, tf.Tensor):\n",
    "        input_image = tf.convert_to_tensor(input_image)\n",
    "    \n",
    "    # Create a baseline (black image) if not provided\n",
    "    if baseline is None:\n",
    "        baseline = tf.zeros_like(input_image)\n",
    "    \n",
    "    # Calculate the path from baseline to input\n",
    "    path = [baseline + (i / steps) * (input_image - baseline) for i in range(1, steps + 1)]\n",
    "    path_tensor = tf.stack(path)\n",
    "    \n",
    "    # Compute gradients at each step\n",
    "    gradients = []\n",
    "    \n",
    "    # For each step along the path\n",
    "    for step_input in path:\n",
    "        # Use GradientTape to compute gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(step_input)\n",
    "            prediction = model(step_input)\n",
    "            \n",
    "            # Use predicted class if target_class is not specified\n",
    "            if target_class is None:\n",
    "                target_class = tf.argmax(prediction[0])\n",
    "                \n",
    "            output = prediction[:, target_class]\n",
    "        \n",
    "        # Get the gradient\n",
    "        gradient = tape.gradient(output, step_input)\n",
    "        gradients.append(gradient)\n",
    "    \n",
    "    # Stack gradients\n",
    "    gradients_tensor = tf.stack(gradients)\n",
    "    \n",
    "    # Riemann sum approximation of the integral\n",
    "    integrated_gradients = tf.reduce_mean(gradients_tensor, axis=0)\n",
    "    \n",
    "    # Multiply by (input - baseline) for final attribution\n",
    "    attribution = integrated_gradients * (input_image - baseline)\n",
    "    \n",
    "    return attribution.numpy()\n",
    "\n",
    "# Generate integrated gradients explanation\n",
    "ig_explanation = integrated_gradients(model_no_softmax, preprocessed_img, target_class=predicted_class, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize integrated gradients explanation compared to standard gradient\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Standard gradient\n",
    "visualize_attribution(np_img, preprocess_explanation(gradient_explanation), ax=axes[0])\n",
    "axes[0].set_title(\"Standard Gradient\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Integrated gradients\n",
    "visualize_attribution(np_img, preprocess_explanation(ig_explanation), ax=axes[1])\n",
    "axes[1].set_title(\"Integrated Gradients (20 steps)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this advanced tutorial, we explored several sophisticated aspects of using SignXAI with TensorFlow:\n",
    "\n",
    "1. **Customizing LRP Rules**: We demonstrated how to configure different LRP variants including Z-rule, epsilon-rule, alpha-beta rules, and composite LRP configurations.\n",
    "\n",
    "2. **Creating Custom Explainability Methods**: We implemented custom methods like thresholded gradient and manually implemented Integrated Gradients.\n",
    "\n",
    "3. **Working with Custom Models**: We showed how SignXAI works seamlessly with custom model architectures.\n",
    "\n",
    "4. **Advanced Visualization**: We explored different visualization options including different colormaps and occlusion sensitivity analysis.\n",
    "\n",
    "5. **Integrating with GradientTape**: We demonstrated how to use TensorFlow's GradientTape API to create custom explainability methods.\n",
    "\n",
    "These advanced techniques provide deeper insights into how your models make decisions and can help identify potential biases or weaknesses in the model's reasoning process.\n",
    "\n",
    "For more information and to contribute to the project, visit the SignXAI documentation and GitHub repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}