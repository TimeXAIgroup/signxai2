{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SignXAI2 PyTorch Tutorial - Image Classification\n",
    "\n",
    "This tutorial demonstrates how to use SignXAI2 for explaining image classification models with PyTorch.\n",
    "\n",
    "## Setup\n",
    "\n",
    "⚠️ **Data Requirements**: This tutorial requires example data from the GitHub repository. Please ensure you have downloaded the necessary data files or cloned the repository.\n",
    "\n",
    "First, let's download the signxai2 package and a sample image to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the signxai2 package if not already installed\n",
    " !pip install signxai2[pytorch]\n",
    "\n",
    "# Download an example image\n",
    "import urllib.request\n",
    "\n",
    "# Download an image of a dog\n",
    "url = \"http://vision.stanford.edu/aditya86/ImageNetDogs/images/n02106030-collie/n02106030_16370.jpg\"\n",
    "urllib.request.urlretrieve(url, \"dog.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Implementation\n",
    "\n",
    "Now let's do the same with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from signxai import explain, list_methods\n",
    "from signxai.utils.utils import normalize_heatmap\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Remove softmax layer (critical for explanations)\n",
    "model.classifier[-1] = torch.nn.Identity()\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = \"dog.jpg\"\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "img_np = np.array(img.resize((224, 224))) / 255.0  # For visualization\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted_idx = torch.max(output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate explanations with different methods\nmethods = [\n    \"gradient\",\n    \"gradient_x_input\",\n    \"integrated_gradients\",\n    \"smoothgrad\",\n    \"grad_cam\",\n    \"lrp_z\",\n    \"lrp_epsilon_0_1\",\n    \"lrpsign_z\"  # The SIGN method\n]\n\nexplanations = {}\nfor method in methods:\n    explanations[method] = explain(\n        model=model,\n        x=input_tensor,\n        method_name=method,\n        target_class=predicted_idx.item()\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explanations\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Original image\n",
    "axs[0].imshow(img_np)\n",
    "axs[0].set_title('Original Image', fontsize=14)\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Explanations\n",
    "for i, method in enumerate(methods[:7]):\n",
    "    explanation = explanations[method][0].sum(axis=0)\n",
    "    axs[i+1].imshow(normalize_heatmap(explanation), cmap='seismic', clim=(-1, 1))\n",
    "    axs[i+1].set_title(method, fontsize=14)\n",
    "    axs[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Highlight the difference between standard LRP and SIGN\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(img_np)\nplt.title('Original Image', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\n# For PyTorch, we need to handle the tensor format\nlrp_z_expl = explanations['lrp_z']\nif hasattr(lrp_z_expl, 'detach'):\n    lrp_z_expl = lrp_z_expl.detach().cpu().numpy()\nif lrp_z_expl.ndim == 4:\n    lrp_z_expl = lrp_z_expl[0]\nif lrp_z_expl.shape[0] == 3:  # CHW format\n    lrp_z_expl = lrp_z_expl.transpose(1, 2, 0)\nlrp_z_heatmap = lrp_z_expl.sum(axis=-1) if lrp_z_expl.ndim == 3 else lrp_z_expl\nplt.imshow(normalize_heatmap(lrp_z_heatmap), cmap='seismic', clim=(-1, 1))\nplt.title('LRP-Z', fontsize=14)\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\n# For PyTorch, we need to handle the tensor format\nlrpsign_z_expl = explanations['lrpsign_z']\nif hasattr(lrpsign_z_expl, 'detach'):\n    lrpsign_z_expl = lrpsign_z_expl.detach().cpu().numpy()\nif lrpsign_z_expl.ndim == 4:\n    lrpsign_z_expl = lrpsign_z_expl[0]\nif lrpsign_z_expl.shape[0] == 3:  # CHW format\n    lrpsign_z_expl = lrpsign_z_expl.transpose(1, 2, 0)\nlrpsign_z_heatmap = lrpsign_z_expl.sum(axis=-1) if lrpsign_z_expl.ndim == 3 else lrpsign_z_expl\nplt.imshow(normalize_heatmap(lrpsign_z_heatmap), cmap='seismic', clim=(-1, 1))\nplt.title('LRP-SIGN', fontsize=14)\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
